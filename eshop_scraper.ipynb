{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ›’ UniverzÃ¡lnÃ­ E-shop Scraper\n",
        "\n",
        "NÃ¡stroj pro stahovÃ¡nÃ­ produktovÃ½ch dat z e-shopÅ¯ do Excelu.\n",
        "\n",
        "## NÃ¡vod:\n",
        "1. **BuÅˆka 1** - Instalace (spusÅ¥te jednou)\n",
        "2. **BuÅˆka 2** - Zadejte URL webu\n",
        "3. **BuÅˆka 3** - SpusÅ¥te scraping\n",
        "4. **BuÅˆka 4** - StÃ¡hnÄ›te Excel\n",
        "5. **BuÅˆka 5** - Reset pro novÃ½ web\n",
        "\n",
        "ğŸ’¡ **Tip:** MÅ¯Å¾ete kdykoliv zastavit (â¹ï¸) a stÃ¡hnout ÄÃ¡steÄnÃ© vÃ½sledky."
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "#@title ğŸ“¦ BuÅˆka 1: Instalace\n",
        "!pip install requests beautifulsoup4 pandas openpyxl lxml -q\n",
        "print(\"âœ… Instalace dokonÄena\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸŒ BuÅˆka 2: NastavenÃ­ webu\n",
        "URL_WEBU = \"https://aktin.cz\" #@param {type:\"string\"}\n",
        "\n",
        "# VyÄistit URL\n",
        "URL_WEBU = URL_WEBU.strip().rstrip('/')\n",
        "print(f\"ğŸ¯ CÃ­lovÃ½ web: {URL_WEBU}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸš€ BuÅˆka 3: Scraping (lze zastavit)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GLOBÃLNÃ - pÅ™eÅ¾ijÃ­ zastavenÃ­\n",
        "if 'products_data' not in dir(): products_data = []\n",
        "if 'all_product_urls' not in dir(): all_product_urls = set()\n",
        "if 'processed_urls' not in dir(): processed_urls = set()\n",
        "if 'visited_pages' not in dir(): visited_pages = set()\n",
        "\n",
        "BASE_URL = URL_WEBU\n",
        "DOMAIN = urlparse(BASE_URL).netloc\n",
        "\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120.0.0.0',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',\n",
        "]\n",
        "\n",
        "HEADERS = {\n",
        "    'User-Agent': random.choice(USER_AGENTS),\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'cs-CZ,cs;q=0.9,en;q=0.8',\n",
        "}\n",
        "\n",
        "KNOWN_CATEGORIES = {\n",
        "    'aktin.cz': ['/proteiny', '/aminokyseliny', '/kreatin', '/gainery', '/spalovace-tuku',\n",
        "                 '/vitaminy-mineraly', '/zdravi', '/superfood', '/orechova-masla', '/snacky',\n",
        "                 '/napoje', '/potraviny', '/tycinky', '/pomucky', '/obleceni', '/novinky'],\n",
        "    'brainmarket.cz': ['/brainmax-doplnky-stravy/', '/brainmax-pure/', '/doplnky-stravy/',\n",
        "                       '/potraviny-bm/', '/kosmetika-drogerie/', '/novinky/'],\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "def get_page(url, retries=3):\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            session.headers['User-Agent'] = random.choice(USER_AGENTS)\n",
        "            r = session.get(url, timeout=30)\n",
        "            if r.status_code == 200: return r.text\n",
        "            time.sleep(3)\n",
        "        except: time.sleep(3)\n",
        "    return None\n",
        "\n",
        "def clean_price(t):\n",
        "    if not t: return \"\"\n",
        "    p = re.sub(r'[^\\d,.]', '', str(t)).replace(',', '.')\n",
        "    return p\n",
        "\n",
        "def clean_text(t):\n",
        "    if not isinstance(t, str): return str(t) if t else \"\"\n",
        "    return re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', t).strip()\n",
        "\n",
        "def is_product_url(url):\n",
        "    if not url or DOMAIN not in url: return False\n",
        "    excluded = ['/kosik', '/cart', '/login', '/registrace', '/kontakt', '/blog',\n",
        "                '/podminky', '/gdpr', '/sitemap', '/search', '/kategorie', '/strana-',\n",
        "                '.pdf', '.jpg', '.png', '/admin', '/api']\n",
        "    path = urlparse(url).path.lower()\n",
        "    for e in excluded:\n",
        "        if e in path: return False\n",
        "    return len(path) > 5\n",
        "\n",
        "def find_links(soup, base):\n",
        "    urls = set()\n",
        "    selectors = ['a.p-name', '.product a', '.products a', 'h2 a', 'h3 a', '.item a', '.card a']\n",
        "    for sel in selectors:\n",
        "        for a in soup.select(sel):\n",
        "            href = a.get('href', '')\n",
        "            if href:\n",
        "                full = urljoin(base, href).split('#')[0]\n",
        "                if is_product_url(full): urls.add(full)\n",
        "    return urls\n",
        "\n",
        "def extract_data(url):\n",
        "    html = get_page(url)\n",
        "    if not html: return None\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    data = {'nazev': '', 'ean': '', 'cena': '', 'cena_puvodni': '', 'sleva': '', 'dostupnost': '', 'url': url}\n",
        "    \n",
        "    # NÃ¡zev\n",
        "    for sel in ['h1', '[itemprop=\"name\"]', '.product-title']:\n",
        "        el = soup.select_one(sel)\n",
        "        if el:\n",
        "            data['nazev'] = clean_text(el.get_text(strip=True))\n",
        "            if data['nazev']: break\n",
        "    if not data['nazev']: return None\n",
        "    \n",
        "    # EAN\n",
        "    for script in soup.select('script[type=\"application/ld+json\"]'):\n",
        "        try:\n",
        "            j = json.loads(script.string or '{}')\n",
        "            m = re.search(r'\"gtin13?\"\\s*:\\s*\"?(\\d{8,14})\"?', json.dumps(j))\n",
        "            if m: data['ean'] = m.group(1); break\n",
        "        except: pass\n",
        "    if not data['ean']:\n",
        "        m = re.search(r'EAN[:\\s]*(\\d{8,14})', html, re.I)\n",
        "        if m: data['ean'] = m.group(1)\n",
        "    \n",
        "    # Cena\n",
        "    for sel in ['[itemprop=\"price\"]', '.price-final', '.p-final', '.price', '.current-price']:\n",
        "        el = soup.select_one(sel)\n",
        "        if el:\n",
        "            data['cena'] = clean_price(el.get('content') or el.get_text())\n",
        "            if data['cena']: break\n",
        "    \n",
        "    # PÅ¯vodnÃ­ cena\n",
        "    for sel in ['.price-standard', '.p-standard', '.old-price', 'del']:\n",
        "        el = soup.select_one(sel)\n",
        "        if el:\n",
        "            data['cena_puvodni'] = clean_price(el.get_text())\n",
        "            if data['cena_puvodni']: break\n",
        "    \n",
        "    # Sleva\n",
        "    if data['cena'] and data['cena_puvodni']:\n",
        "        try:\n",
        "            c, o = float(data['cena']), float(data['cena_puvodni'])\n",
        "            if o > c: data['sleva'] = f\"{((o-c)/o)*100:.0f}%\"\n",
        "        except: pass\n",
        "    \n",
        "    # Dostupnost\n",
        "    el = soup.select_one('.availability, .p-availability, .stock')\n",
        "    if el: data['dostupnost'] = clean_text(el.get_text(strip=True))\n",
        "    \n",
        "    return data\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"ğŸ›’ E-SHOP SCRAPER\")\n",
        "print(f\"ğŸ¯ Web: {BASE_URL}\")\n",
        "print(f\"ğŸ“Š StaÅ¾eno: {len(products_data)} | URL: {len(all_product_urls)}\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ’¡ Stop = â¹ï¸ | Pak BuÅˆka 4 pro staÅ¾enÃ­\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # FÃ¡ze 1: SbÃ­rÃ¡nÃ­ URL\n",
        "    if not all_product_urls:\n",
        "        print(\"\\nğŸ“ FÃ¡ze 1: HledÃ¡nÃ­ produktÅ¯\\n\")\n",
        "        to_visit = {BASE_URL}\n",
        "        for d, cats in KNOWN_CATEGORIES.items():\n",
        "            if d in DOMAIN:\n",
        "                to_visit.update([BASE_URL + c for c in cats])\n",
        "        \n",
        "        while to_visit and len(visited_pages) < 500:\n",
        "            url = to_visit.pop()\n",
        "            if url in visited_pages: continue\n",
        "            print(f\"   [{len(visited_pages)+1}] {url[:55]}...\", end=\" \", flush=True)\n",
        "            html = get_page(url)\n",
        "            if not html: print(\"âŒ\"); visited_pages.add(url); continue\n",
        "            soup = BeautifulSoup(html, 'html.parser')\n",
        "            visited_pages.add(url)\n",
        "            new = find_links(soup, url)\n",
        "            before = len(all_product_urls)\n",
        "            all_product_urls.update(new)\n",
        "            print(f\"âœ… +{len(all_product_urls)-before} ({len(all_product_urls)})\")\n",
        "            # Pagination\n",
        "            for a in soup.select('a.next, a[rel=\"next\"], .pagination a'):\n",
        "                href = a.get('href')\n",
        "                if href:\n",
        "                    full = urljoin(url, href)\n",
        "                    if DOMAIN in full and full not in visited_pages:\n",
        "                        to_visit.add(full)\n",
        "            time.sleep(random.uniform(0.5, 1.5))\n",
        "        print(f\"\\nğŸ“Š Nalezeno {len(all_product_urls)} URL\")\n",
        "    \n",
        "    # FÃ¡ze 2: StahovÃ¡nÃ­\n",
        "    print(\"\\nğŸ“¦ FÃ¡ze 2: StahovÃ¡nÃ­ detailÅ¯\\n\")\n",
        "    todo = list(all_product_urls - processed_urls)\n",
        "    total = len(todo)\n",
        "    for i, url in enumerate(todo, 1):\n",
        "        print(f\"\\r   [{i}/{total}] {(i/total)*100:.1f}% | ProduktÅ¯: {len(products_data)}\", end=\"\", flush=True)\n",
        "        data = extract_data(url)\n",
        "        if data: products_data.append(data)\n",
        "        processed_urls.add(url)\n",
        "        time.sleep(random.uniform(0.5, 1.2))\n",
        "        if i % 50 == 0:\n",
        "            pd.DataFrame(products_data).to_excel('/content/temp.xlsx', index=False)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nâ¹ï¸ ZASTAVENO\")\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"âœ… StaÅ¾eno: {len(products_data)} produktÅ¯\")\n",
        "print(f\"   S EAN: {len([p for p in products_data if p['ean']])}\")\n",
        "print(f\"   ZbÃ½vÃ¡: {len(all_product_urls) - len(processed_urls)}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "scraper"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸ’¾ BuÅˆka 4: StaÅ¾enÃ­ Excel\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def clean(t):\n",
        "    if not isinstance(t, str): return str(t) if t else \"\"\n",
        "    return re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', t).strip()\n",
        "\n",
        "if 'products_data' in dir() and products_data:\n",
        "    data = [{k: clean(v) for k, v in p.items()} for p in products_data]\n",
        "    df = pd.DataFrame(data)\n",
        "    df.columns = ['NÃ¡zev produktu', 'EAN', 'Cena', 'PÅ¯vodnÃ­ cena', 'Sleva', 'Dostupnost', 'URL']\n",
        "    df = df.drop_duplicates(subset=['NÃ¡zev produktu', 'URL']).sort_values('NÃ¡zev produktu')\n",
        "    \n",
        "    domain = DOMAIN.replace('www.', '').replace('.', '_')\n",
        "    fn = f'{domain}_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.xlsx'\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ“Š Export: {len(df)} produktÅ¯\")\n",
        "    print(f\"   S EAN: {len(df[df['EAN'].str.len() > 0])}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    df.to_excel(fn, index=False)\n",
        "    files.download(fn)\n",
        "else:\n",
        "    print(\"âŒ Å½Ã¡dnÃ¡ data\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸ”„ BuÅˆka 5: Reset pro novÃ½ web\n",
        "products_data = []\n",
        "all_product_urls = set()\n",
        "processed_urls = set()\n",
        "visited_pages = set()\n",
        "print(\"ğŸ”„ Reset - zmÄ›Åˆte URL v BuÅˆce 2\")"
      ],
      "metadata": {
        "id": "reset"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
