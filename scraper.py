# =============================================================================
# üõí UNIVERZ√ÅLN√ç E-SHOP SCRAPER
# =============================================================================
# Stahuje produktov√° data (n√°zev, EAN, cena, dostupnost) z e-shop≈Ø
# Optimalizov√°no pro: Shoptet, WooCommerce, PrestaShop, Shopify
# =============================================================================
# N√ÅVOD PRO GOOGLE COLAB:
#   1. Bu≈àka 1: Instalace (pip install...)
#   2. Bu≈àka 2: Nastavte URL_WEBU
#   3. Bu≈àka 3: Zkop√≠rujte tento cel√Ω soubor
#   4. Bu≈àka 4: Sta≈æen√≠ v√Ωsledk≈Ø
#   5. Bu≈àka 5: Reset pro nov√Ω web
# =============================================================================

# =============================================================================
# BU≈áKA 1: INSTALACE (spus≈•te jednou)
# =============================================================================
# !pip install requests beautifulsoup4 pandas openpyxl lxml -q
# print("‚úÖ Instalace dokonƒçena")

# =============================================================================
# BU≈áKA 2: NASTAVEN√ç WEBU
# =============================================================================
# URL_WEBU = "https://aktin.cz"  # Zmƒõ≈àte na v√°≈° c√≠lov√Ω web

# =============================================================================
# BU≈áKA 3: HLAVN√ç SCRAPER (zkop√≠rujte cel√©)
# =============================================================================

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import json
import random
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ===========================================================================
# GLOB√ÅLN√ç PROMƒöNN√â - p≈ôe≈æij√≠ zastaven√≠!
# ===========================================================================
if 'products_data' not in dir(): products_data = []
if 'all_product_urls' not in dir(): all_product_urls = set()
if 'processed_urls' not in dir(): processed_urls = set()
if 'visited_pages' not in dir(): visited_pages = set()
if 'category_urls' not in dir(): category_urls = set()

# ===========================================================================
# KONFIGURACE
# ===========================================================================
try:
    BASE_URL = URL_WEBU.strip().rstrip('/')
except:
    BASE_URL = "https://aktin.cz"  # V√Ωchoz√≠ hodnota

DOMAIN = urlparse(BASE_URL).netloc

# Roz≈°√≠≈ôen√© headers pro obch√°zen√≠ blokac√≠
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
]

HEADERS = {
    'User-Agent': random.choice(USER_AGENTS),
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'cs-CZ,cs;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Cache-Control': 'max-age=0',
}

# Nastaven√≠
DELAY_MIN = 0.5
DELAY_MAX = 1.5
MAX_PAGES = 1000
MAX_PRODUCTS = 100000
MAX_RETRIES = 3

# Zn√°m√© kategorie pro r≈Øzn√© e-shopy (roz≈°i≈ôiteln√©)
KNOWN_CATEGORIES = {
    'aktin.cz': [
        '/proteiny', '/aminokyseliny', '/kreatin', '/sacharidy', '/gainery',
        '/spalovace-tuku', '/vitaminy-mineraly', '/zdravi', '/superfood',
        '/orechova-masla', '/snacky', '/napoje', '/potraviny', '/tycinky',
        '/kloubni-vyziva', '/imunita', '/trava-a-bylinky', '/pece-o-telo',
        '/pomucky', '/obleceni', '/balicky', '/novinky', '/sleva', '/vyprodej',
    ],
    'brainmarket.cz': [
        '/brainmax-doplnky-stravy/', '/brainmax-pure/', '/doplnky-stravy/',
        '/potraviny-bm/', '/domov/', '/kosmetika-drogerie/', '/obleceni-3/',
        '/trainmax/', '/wellmax/', '/lauf/', '/blight/', '/usetri/', '/novinky/',
    ],
}

# ===========================================================================
# POMOCN√â FUNKCE
# ===========================================================================

session = requests.Session()
session.headers.update(HEADERS)

def get_delay():
    """N√°hodn√© zpo≈ædƒõn√≠ mezi po≈æadavky"""
    return random.uniform(DELAY_MIN, DELAY_MAX)

def get_page(url, retries=MAX_RETRIES):
    """St√°hne str√°nku s opakov√°n√≠m a rotac√≠ User-Agent"""
    for i in range(retries):
        try:
            # Rotace User-Agent
            session.headers['User-Agent'] = random.choice(USER_AGENTS)
            
            response = session.get(url, timeout=30, allow_redirects=True)
            
            if response.status_code == 200:
                return response.text
            elif response.status_code == 403:
                print(f"\n    ‚ö†Ô∏è Blokov√°no (403), zkou≈°√≠m znovu...")
                time.sleep(5 * (i + 1))
            elif response.status_code == 429:
                print(f"\n    ‚ö†Ô∏è Rate limit, ƒçek√°m...")
                time.sleep(30)
            else:
                time.sleep(2)
        except Exception as e:
            time.sleep(3 * (i + 1))
    return None

def clean_price(text):
    """Vyƒçist√≠ cenu"""
    if not text:
        return ""
    # Odstran√≠ mƒõnu a mezery
    price = re.sub(r'[^\d,.]', '', str(text))
    # Nahrad√≠ ƒç√°rku teƒçkou
    price = price.replace(',', '.')
    # Ponech√° jen posledn√≠ teƒçku (pro desetinn√° m√≠sta)
    parts = price.rsplit('.', 1)
    if len(parts) == 2 and len(parts[1]) <= 2:
        price = parts[0].replace('.', '') + '.' + parts[1]
    else:
        price = price.replace('.', '')
    return price

def clean_text(text):
    """Odstran√≠ neplatn√© znaky pro Excel"""
    if not isinstance(text, str):
        return str(text) if text else ""
    # Odstran√≠ kontroln√≠ znaky
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    # Odstran√≠ speci√°ln√≠ Unicode znaky kter√© dƒõlaj√≠ probl√©m
    text = text.encode('utf-8', errors='ignore').decode('utf-8')
    return text.strip()

def is_product_url(url):
    """Heuristika - je to URL produktu?"""
    if not url or not url.startswith(('http://', 'https://')):
        return False
    
    parsed = urlparse(url)
    if DOMAIN not in parsed.netloc:
        return False
    
    path = parsed.path.lower()
    
    # Vylouƒçit syst√©mov√© str√°nky
    excluded_patterns = [
        # Ko≈°√≠k a objedn√°vky
        '/kosik', '/cart', '/basket', '/checkout', '/objednavka', '/order', '/pokladna',
        # √öƒçet
        '/login', '/prihlaseni', '/registrace', '/register', '/ucet', '/account', '/profil',
        '/zapomenute-heslo', '/odhlaseni', '/logout',
        # Informaƒçn√≠ str√°nky
        '/kontakt', '/contact', '/o-nas', '/about', '/o-spolecnosti', '/firma',
        '/blog', '/clanek', '/article', '/magazin', '/clanky', '/recepty',
        '/podminky', '/terms', '/gdpr', '/cookies', '/ochrana-udaju', '/privacy',
        '/faq', '/pomoc', '/help', '/otazky', '/zakaznicka-podpora',
        '/doprava', '/shipping', '/platba', '/payment', '/reklamace', '/return', '/vraceni',
        '/jak-nakupovat', '/obchodni-podminky', '/vse-o-nakupu',
        # Technick√©
        '/sitemap', '/feed', '/rss', '/xml', '/json', '/api', '/ajax', '/graphql',
        '/search', '/hledat', '/vyhledavani',
        '/tag', '/znacka', '/brand', '/vyrobce', '/manufacturer',
        '/kategorie', '/category', '/catalog', '/katalog',
        '/page/', '/strana-', '/stranka-', '?page=', '&page=',
        '/wp-admin', '/admin', '/wp-content', '/wp-includes', '/assets', '/static',
        # Soubory
        '.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.xml', '.ico',
        # Speci√°ln√≠
        '/wishlist', '/porovnani', '/compare', '/hodnoceni', '/review',
        '/sluzby', '/services', '/prodejny', '/stores', '/pobocky',
        '/kariera', '/career', '/spoluprace', '/affiliate', '/partneri',
    ]
    
    for excl in excluded_patterns:
        if excl in path or excl in url.lower():
            return False
    
    # URL nesm√≠ b√Ωt p≈ô√≠li≈° kr√°tk√°
    if len(path) < 5 or path == '/':
        return False
    
    # Produkt m√° obvykle del≈°√≠ URL s n√°zvem
    # A neobsahuje v√≠ce ne≈æ jeden parametr
    if url.count('?') > 1:
        return False
    
    return True

def is_category_url(url):
    """Je to URL kategorie?"""
    if not url or not url.startswith(('http://', 'https://')):
        return False
    
    parsed = urlparse(url)
    if DOMAIN not in parsed.netloc:
        return False
    
    path = parsed.path.lower()
    
    # Vylouƒçit syst√©mov√© str√°nky (striktnƒõj≈°√≠ filtr)
    excluded = [
        '/kosik', '/cart', '/checkout', '/login', '/registrace', '/account',
        '/kontakt', '/blog', '/clanek', '/podminky', '/gdpr', '/faq', '/sitemap',
        '.pdf', '.jpg', '.png', '/wp-admin', '/admin', '/api',
        '/objednavka', '/order', '/prihlaseni', '/odhlaseni',
    ]
    
    for excl in excluded:
        if excl in path:
            return False
    
    return True

def get_known_categories():
    """Vr√°t√≠ zn√°m√© kategorie pro dan√Ω web"""
    for domain, categories in KNOWN_CATEGORIES.items():
        if domain in DOMAIN:
            return [BASE_URL + cat for cat in categories]
    return []

def find_product_links(soup, base_url):
    """Najde odkazy na produkty na str√°nce"""
    urls = set()
    
    # Shoptet specifick√© selektory
    shoptet_selectors = [
        'a.p-name', 'a.p-item-title', '.p-item a.p-name',
        '.p-info a', '.product-name a', 'a.product-name',
        '.p h2 a', '.p h3 a', '.p-item h2 a',
        'a[data-product-name]', '[data-product] a',
    ]
    
    # WooCommerce selektory
    woo_selectors = [
        '.woocommerce-loop-product__link',
        '.woocommerce-LoopProduct-link',
        'ul.products li.product a',
        '.product-item-link', '.product a.product-item-link',
    ]
    
    # PrestaShop selektory
    presta_selectors = [
        '.product-title a', '.product_name a',
        '.product-miniature a.thumbnail',
        '.product-container a.product-name',
    ]
    
    # Shopify selektory
    shopify_selectors = [
        '.product-card a', '.product-card__link',
        '.product-item a', '.product-link',
        '.grid-product__link', '.card__link',
    ]
    
    # Obecn√© selektory
    generic_selectors = [
        '.product a', '.products a', '[class*="product"] a',
        '.item a', '.card a', '.grid-item a',
        'h2 a', 'h3 a', 'h4 a',
        'article a', '.product-list a',
        '.collection-product a', '.product-grid a',
    ]
    
    all_selectors = (shoptet_selectors + woo_selectors + 
                     presta_selectors + shopify_selectors + generic_selectors)
    
    for selector in all_selectors:
        try:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and not href.startswith('#') and not href.startswith('javascript:'):
                    full_url = urljoin(base_url, href)
                    # Odstranit fragment a normalizovat
                    full_url = full_url.split('#')[0]
                    if is_product_url(full_url):
                        urls.add(full_url)
        except:
            pass
    
    # Z√°lo≈æn√≠ metoda - v≈°echny odkazy
    if len(urls) < 3:
        for a in soup.find_all('a', href=True):
            href = a.get('href', '')
            if href and not href.startswith('#'):
                full_url = urljoin(base_url, href)
                full_url = full_url.split('#')[0]
                if is_product_url(full_url):
                    urls.add(full_url)
    
    return urls

def find_category_links(soup, base_url):
    """Najde odkazy na kategorie a podkategorie"""
    urls = set()
    
    category_selectors = [
        # Navigace
        'nav a', '.menu a', '.navigation a', '.navbar a', 'header a',
        '.main-menu a', '.primary-menu a', '.site-nav a',
        # Kategorie
        '.category a', '.categories a', '[class*="category"] a',
        '.cat-item a', '.product-category a', '.subcategory a',
        # Sidebar
        '.sidebar a', '.widget a', '.aside a',
        # Shoptet
        '.category-tree a', '.p-category-list a', '.navigation-categories a',
        # Obecn√©
        '.nav a', 'ul.menu a', 'li.menu-item a', '.dropdown a',
    ]
    
    for selector in category_selectors:
        try:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and not href.startswith('#'):
                    full_url = urljoin(base_url, href)
                    full_url = full_url.split('#')[0]
                    if is_category_url(full_url) and full_url not in visited_pages:
                        urls.add(full_url)
        except:
            pass
    
    return urls

def find_pagination_links(soup, base_url):
    """Najde odkazy na dal≈°√≠ str√°nky"""
    urls = set()
    
    pagination_selectors = [
        # P≈ô√≠m√© next odkazy
        'a.next', 'a[rel="next"]', '.next a', '.pagination-next a',
        'a[title*="dal≈°√≠"]', 'a[title*="Dal≈°√≠"]', 'a[title*="Next"]',
        'a[aria-label*="next"]', 'a[aria-label*="dal≈°√≠"]',
        # Str√°nkov√°n√≠
        '.pagination a', '.paging a', '.page-numbers a',
        '.paginator a', '.pages a', '.page-link',
        # Shoptet
        '.paging-list a', '.pagination-list a',
        # WooCommerce
        '.woocommerce-pagination a',
    ]
    
    for selector in pagination_selectors:
        try:
            for link in soup.select(selector):
                href = link.get('href', '')
                if href and not href.startswith('#'):
                    full_url = urljoin(base_url, href)
                    full_url = full_url.split('#')[0]
                    if DOMAIN in full_url and full_url not in visited_pages:
                        urls.add(full_url)
        except:
            pass
    
    return urls

def extract_product_data(url):
    """Extrahuje data z produktov√© str√°nky"""
    html = get_page(url)
    if not html:
        return None
    
    soup = BeautifulSoup(html, 'html.parser')
    
    data = {
        'nazev': '',
        'ean': '',
        'cena': '',
        'cena_puvodni': '',
        'sleva': '',
        'dostupnost': '',
        'url': url,
    }
    
    # === N√ÅZEV ===
    name_selectors = [
        'h1', 'h1.product-title', 'h1.product-name', 'h1.product_title',
        '[itemprop="name"]', '.p-detail-title', '.p-detail-inner h1',
        '.product-title', '.product-name', '.entry-title',
        '.product-detail h1', '.product-info h1', '.product-header h1',
        'h1.title', 'h1.name', '[data-product-name]',
    ]
    
    for sel in name_selectors:
        try:
            el = soup.select_one(sel)
            if el:
                # Preferuj atribut nebo p≈ô√≠m√Ω text
                text = el.get('content') or el.get('data-product-name') or el.get_text(strip=True)
                if text and len(text) > 2 and len(text) < 500:
                    data['nazev'] = clean_text(text)
                    break
        except:
            pass
    
    if not data['nazev']:
        return None
    
    # === EAN / GTIN ===
    # 1. JSON-LD strukturovan√° data
    for script in soup.select('script[type="application/ld+json"]'):
        try:
            json_text = script.string or ''
            if not json_text.strip():
                continue
            json_data = json.loads(json_text)
            
            def find_ean_recursive(obj):
                if isinstance(obj, dict):
                    for key in ['gtin13', 'gtin', 'gtin8', 'gtin12', 'gtin14', 'ean', 'mpn', 'sku', 'productID']:
                        if key in obj and obj[key]:
                            val = str(obj[key]).strip()
                            if re.match(r'^\d{8,14}$', val):
                                return val
                    for v in obj.values():
                        result = find_ean_recursive(v)
                        if result:
                            return result
                elif isinstance(obj, list):
                    for item in obj:
                        result = find_ean_recursive(item)
                        if result:
                            return result
                return None
            
            ean = find_ean_recursive(json_data)
            if ean:
                data['ean'] = ean
                break
        except:
            pass
    
    # 2. Meta tagy
    if not data['ean']:
        meta_selectors = [
            'meta[itemprop="gtin13"]', 'meta[itemprop="gtin"]', 'meta[itemprop="gtin8"]',
            'meta[itemprop="ean"]', 'meta[property="product:ean"]', 'meta[property="og:ean"]',
            'meta[name="ean"]', 'meta[name="gtin"]',
        ]
        for sel in meta_selectors:
            try:
                el = soup.select_one(sel)
                if el and el.get('content'):
                    val = el.get('content').strip()
                    if re.match(r'^\d{8,14}$', val):
                        data['ean'] = val
                        break
            except:
                pass
    
    # 3. Data atributy
    if not data['ean']:
        for attr in ['data-ean', 'data-gtin', 'data-gtin13', 'data-barcode', 'data-product-ean']:
            try:
                el = soup.select_one(f'[{attr}]')
                if el:
                    val = el.get(attr, '').strip()
                    if re.match(r'^\d{8,14}$', val):
                        data['ean'] = val
                        break
            except:
                pass
    
    # 4. Tabulka parametr≈Ø
    if not data['ean']:
        param_containers = soup.select('table, .params, .product-params, .parameters, '
                                        '.specifications, .attributes, dl, .p-params, '
                                        '.product-properties, .product-attributes')
        for container in param_containers:
            try:
                text = container.get_text(separator=' ')
                match = re.search(r'(?:EAN|GTIN|ƒå√°rov√Ω\s*k√≥d|Barcode)[:\s]*(\d{8,14})', text, re.I)
                if match:
                    data['ean'] = match.group(1)
                    break
            except:
                pass
    
    # 5. Regex v cel√©m HTML
    if not data['ean']:
        patterns = [
            r'"gtin13"\s*:\s*"?(\d{13})"?',
            r'"gtin"\s*:\s*"?(\d{8,14})"?',
            r'"ean"\s*:\s*"?(\d{8,14})"?',
            r'data-ean="(\d{8,14})"',
            r'data-gtin="(\d{8,14})"',
            r'>EAN[:\s]*(\d{8,14})<',
        ]
        for pattern in patterns:
            try:
                match = re.search(pattern, html)
                if match:
                    data['ean'] = match.group(1)
                    break
            except:
                pass
    
    # === CENA ===
    price_selectors = [
        '[itemprop="price"]', 'meta[itemprop="price"]',
        '.price-final', '.p-final', '.p-detail-price', '.p-main-price',
        '.current-price', '.product-price', '.price', '.price-box .price',
        '.woocommerce-Price-amount', '.amount', 
        '.price-new', '.special-price', '.offer-price', '.sale-price',
        'ins .amount', '.price ins', '.final-price',
        '[data-price]', '.product-price-value',
    ]
    
    for sel in price_selectors:
        try:
            el = soup.select_one(sel)
            if el:
                # Zkus content atribut, data atribut, nebo text
                price = el.get('content') or el.get('data-price') or el.get_text(strip=True)
                cleaned = clean_price(price)
                if cleaned:
                    try:
                        if float(cleaned) > 0:
                            data['cena'] = cleaned
                            break
                    except:
                        pass
        except:
            pass
    
    # === P≈ÆVODN√ç CENA ===
    orig_selectors = [
        '.price-standard', '.p-standard', '.p-before-price',
        '.original-price', '.old-price', '.price-old', '.was-price',
        '.regular-price', '.list-price', '.compare-price',
        'del .amount', '.price del', 'del.price', 's.price', 's .amount',
        '.price-before-discount', '.crossed-price',
    ]
    
    for sel in orig_selectors:
        try:
            el = soup.select_one(sel)
            if el:
                price = clean_price(el.get_text(strip=True))
                if price:
                    try:
                        if float(price) > 0:
                            data['cena_puvodni'] = price
                            break
                    except:
                        pass
        except:
            pass
    
    # === SLEVA ===
    if data['cena'] and data['cena_puvodni']:
        try:
            curr = float(data['cena'])
            orig = float(data['cena_puvodni'])
            if orig > curr > 0:
                discount = ((orig - curr) / orig) * 100
                data['sleva'] = f"{discount:.0f}%"
        except:
            pass
    
    # === DOSTUPNOST ===
    avail_selectors = [
        '.availability', '.p-availability', '.stock', '.stock-status',
        '[itemprop="availability"]', '.in-stock', '.out-of-stock',
        '.product-availability', '.delivery-info', '.skladem', '.dostupnost',
        '.stock-info', '.availability-status', '.product-stock',
        '[data-availability]', '.inventory-status',
    ]
    
    for sel in avail_selectors:
        try:
            el = soup.select_one(sel)
            if el:
                text = el.get('content') or el.get('data-availability') or el.get_text(strip=True)
                if text:
                    data['dostupnost'] = clean_text(text)[:100]  # Omezit d√©lku
                    break
        except:
            pass
    
    return data

def save_progress():
    """Ulo≈æ√≠ pr≈Øbƒõ≈æn√© v√Ωsledky"""
    if products_data:
        try:
            df = pd.DataFrame(products_data)
            df.to_excel('/content/eshop_prubezne.xlsx', index=False)
        except:
            pass

# ===========================================================================
# HLAVN√ç SCRAPING
# ===========================================================================

print("=" * 70)
print("üõí UNIVERZ√ÅLN√ç E-SHOP SCRAPER")
print("=" * 70)
print(f"üéØ Web: {BASE_URL}")
print(f"üìä Ji≈æ sta≈æeno: {len(products_data)} produkt≈Ø")
print(f"üîó URL v pamƒõti: {len(all_product_urls)}")
print(f"üìÅ Nav≈°t√≠veno str√°nek: {len(visited_pages)}")
print("=" * 70)
print("üí° Pro ZASTAVEN√ç kliknƒõte ‚èπÔ∏è Stop")
print("üí° Po zastaven√≠ spus≈•te BU≈áKU 4 pro sta≈æen√≠")
print("üí° Pro pokraƒçov√°n√≠ znovu spus≈•te tuto bu≈àku")
print("=" * 70)

try:
    # =========================================================================
    # F√ÅZE 1: Objevov√°n√≠ str√°nek a URL produkt≈Ø
    # =========================================================================
    if len(all_product_urls) == 0:
        print(f"\nüìÅ F√ÅZE 1: Prozkoum√°v√°n√≠ webu\n")
        
        # Zaƒçneme od hlavn√≠ str√°nky a zn√°m√Ωch kategori√≠
        pages_to_visit = {BASE_URL}
        
        # P≈ôid√°me zn√°m√© kategorie
        known_cats = get_known_categories()
        if known_cats:
            print(f"   üìÇ Nalezeno {len(known_cats)} zn√°m√Ωch kategori√≠")
            pages_to_visit.update(known_cats)
        
        pages_visited_this_run = 0
        
        while pages_to_visit and len(visited_pages) < MAX_PAGES:
            url = pages_to_visit.pop()
            
            if url in visited_pages:
                continue
            
            pages_visited_this_run += 1
            print(f"   [{pages_visited_this_run}|{len(visited_pages)+1}] {url[:65]}...", end=" ", flush=True)
            
            html = get_page(url)
            if not html:
                print("‚ùå")
                visited_pages.add(url)
                time.sleep(get_delay())
                continue
            
            soup = BeautifulSoup(html, 'html.parser')
            visited_pages.add(url)
            
            # Najdi produkty
            new_products = find_product_links(soup, url)
            before = len(all_product_urls)
            all_product_urls.update(new_products)
            added = len(all_product_urls) - before
            
            # Najdi dal≈°√≠ str√°nky k prozkoum√°n√≠
            cat_links = find_category_links(soup, url)
            pag_links = find_pagination_links(soup, url)
            
            new_pages = (cat_links | pag_links) - visited_pages
            pages_to_visit.update(new_pages)
            
            print(f"‚úÖ +{added} (celkem: {len(all_product_urls)}, fronta: {len(pages_to_visit)})")
            
            time.sleep(get_delay())
            
            if len(all_product_urls) >= MAX_PRODUCTS:
                print(f"\n   ‚ö†Ô∏è Dosa≈æen limit {MAX_PRODUCTS} produkt≈Ø")
                break
        
        print(f"\n{'='*70}")
        print(f"üìä F√ÅZE 1 DOKONƒåENA")
        print(f"   Nav≈°t√≠veno str√°nek: {len(visited_pages)}")
        print(f"   Nalezeno URL produkt≈Ø: {len(all_product_urls)}")
        print("=" * 70)
    else:
        print(f"\nüìä Pokraƒçuji - {len(all_product_urls)} URL v pamƒõti\n")
    
    # =========================================================================
    # F√ÅZE 2: Stahov√°n√≠ detail≈Ø produkt≈Ø
    # =========================================================================
    print(f"\nüì¶ F√ÅZE 2: Stahov√°n√≠ detail≈Ø produkt≈Ø\n")
    
    urls_to_process = list(all_product_urls - processed_urls)
    total = len(urls_to_process)
    
    print(f"   Ke zpracov√°n√≠: {total}")
    print(f"   Ji≈æ hotovo: {len(processed_urls)}")
    print(f"   Sta≈æeno produkt≈Ø: {len(products_data)}\n")
    
    if total == 0:
        print("   ‚úÖ V≈°echny URL ji≈æ zpracov√°ny!")
    
    start_time = time.time()
    
    for i, url in enumerate(urls_to_process, 1):
        elapsed = time.time() - start_time
        rate = i / elapsed if elapsed > 0 else 0
        eta = (total - i) / rate if rate > 0 else 0
        
        print(f"\r   [{i}/{total}] {(i/total)*100:.1f}% | "
              f"Produkt≈Ø: {len(products_data)} | "
              f"ETA: {int(eta//60)}m {int(eta%60)}s   ", end="", flush=True)
        
        try:
            data = extract_product_data(url)
            if data and data['nazev']:
                products_data.append(data)
        except Exception as e:
            pass
        
        processed_urls.add(url)
        time.sleep(get_delay())
        
        # Pr≈Øbƒõ≈æn√© ukl√°d√°n√≠ ka≈æd√Ωch 50 produkt≈Ø
        if i % 50 == 0:
            save_progress()

except KeyboardInterrupt:
    print("\n\n‚èπÔ∏è ZASTAVENO U≈ΩIVATELEM")
    save_progress()

# Z√°vƒõreƒçn√° statistika
print(f"\n\n{'='*70}")
print("üìä AKTU√ÅLN√ç STAV")
print("="*70)
print(f"   Web:                 {BASE_URL}")
print(f"   Sta≈æeno produkt≈Ø:    {len(products_data)}")
print(f"   S EAN k√≥dem:         {len([p for p in products_data if p.get('ean')])}")
print(f"   S cenou:             {len([p for p in products_data if p.get('cena')])}")
print(f"   Ve slevƒõ:            {len([p for p in products_data if p.get('sleva')])}")
print(f"   Zpracov√°no URL:      {len(processed_urls)}/{len(all_product_urls)}")
print(f"   Zb√Ωv√°:               {len(all_product_urls) - len(processed_urls)}")
print("="*70)
print("\n‚úÖ Spus≈•te BU≈áKU 4 pro sta≈æen√≠ Excel souboru")
print("üí° Nebo znovu tuto bu≈àku pro pokraƒçov√°n√≠")

session.close()


# =============================================================================
# BU≈áKA 4: STA≈ΩEN√ç V√ùSLEDK≈Æ (spus≈•te kdykoliv)
# =============================================================================
"""
from google.colab import files
from datetime import datetime
from urllib.parse import urlparse
import pandas as pd
import re

def clean_for_excel(text):
    if not isinstance(text, str):
        return str(text) if text else ""
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    return text.strip()

if 'products_data' in dir() and products_data:
    # Vyƒçistit data
    clean_data = []
    for p in products_data:
        clean_p = {k: clean_for_excel(v) for k, v in p.items()}
        clean_data.append(clean_p)
    
    df = pd.DataFrame(clean_data)
    df = df.rename(columns={
        'nazev': 'N√°zev produktu',
        'ean': 'EAN',
        'cena': 'Cena',
        'cena_puvodni': 'P≈Øvodn√≠ cena',
        'sleva': 'Sleva',
        'dostupnost': 'Dostupnost',
        'url': 'URL'
    })
    
    # Odstranit duplik√°ty
    df = df.drop_duplicates(subset=['N√°zev produktu', 'URL'])
    df = df.sort_values('N√°zev produktu')
    
    # N√°zev souboru podle dom√©ny
    domain = urlparse(BASE_URL).netloc.replace('www.', '').replace('.', '_')
    filename = f'{domain}_{datetime.now().strftime("%Y%m%d_%H%M")}.xlsx'
    
    print("="*70)
    print("üìä SOUHRN EXPORTU")
    print("="*70)
    print(f"   Web:              {BASE_URL}")
    print(f"   Celkem produkt≈Ø:  {len(df)}")
    print(f"   S EAN k√≥dem:      {len(df[df['EAN'].astype(str).str.len() > 0])}")
    print(f"   S cenou:          {len(df[df['Cena'].astype(str).str.len() > 0])}")
    print(f"   Ve slevƒõ:         {len(df[df['Sleva'].astype(str).str.len() > 0])}")
    print("="*70)
    
    df.to_excel(filename, index=False)
    files.download(filename)
    print(f"\n‚úÖ Stahuji: {filename}")
else:
    print("‚ùå ≈Ω√°dn√° data - nejd≈ô√≠v spus≈•te BU≈áKU 3")
"""


# =============================================================================
# BU≈áKA 5: RESET (pro nov√Ω web)
# =============================================================================
"""
products_data = []
all_product_urls = set()
processed_urls = set()
visited_pages = set()
category_urls = set()
print("üîÑ Reset dokonƒçen - zmƒõ≈àte URL_WEBU v BU≈áCE 2 a spus≈•te BU≈áKU 3")
"""
